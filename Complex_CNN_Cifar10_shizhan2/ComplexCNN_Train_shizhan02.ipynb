{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import graph_util\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from keras.datasets import cifar10\n",
    "pb_file_path = os.getcwd() + '/'\n",
    "\n",
    "batch_size=128\n",
    "max_steps=3000\n",
    "nb = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "def nextBatch():\n",
    "    global nb\n",
    "    nb = nb + 1\n",
    "    if nb == 390:\n",
    "        nb = 0\n",
    "    return trainX[nb*128:(nb+1)*128], np.reshape(trainY[nb*128:(nb+1)*128], (128,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "###定义权重的函数###\n",
    "#使用截断的正态分布初始化权重变量\n",
    "#如果wl不为0，则对对于权重添加一个L2范数的正则化，且命名为'weight_loss'，\n",
    "#然后将这个正则化部分添加到名为'losses'的collection中，方便后续调用；\n",
    "def variable_with_weight_loss(shape,stddev,wl,name):\n",
    "    var=tf.Variable(tf.truncated_normal(shape,stddev=stddev), name=name)\n",
    "    if wl is not None:\n",
    "        weight_loss=tf.multiply(tf.nn.l2_loss(var),wl,name='weight_loss')\n",
    "        tf.add_to_collection('losses',weight_loss)\n",
    "    return var\n",
    "\n",
    "###定义损失函数，将L2也加入其中###\n",
    "def loss(logits,labels):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    #使用softmax分类器，然后结合交叉熵作为损失函数\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = labels, name = 'cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name = 'cross_entropy')\n",
    "    #将平均后的损失函数也添加进入‘losses’\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    #tf.get_collection('losses')，将之前添加到列表losses中的内容输出成一个数组\n",
    "    #使用tf.add_n，将列表losses中的所有项求和；\n",
    "    return tf.add_n(tf.get_collection('losses'), name = 'total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    #创建输入图像和标签所需要的占位符（placeholder），batch_size是一次输入的图像数目，32*32是裁图像大小，3是图像的通道数（输入为彩色图）\n",
    "    image_holder=tf.placeholder(tf.float32,[batch_size,32,32,3], name='image')\n",
    "    label_holder=tf.placeholder(tf.int32,[batch_size], name='label')\n",
    "\n",
    "    ###创建第一个卷积层####这一层有64个节点\n",
    "    #调用variable_with_weight_loss函数创建第一个卷积核，大小为5*5，通道数为3，标准差为0.05，不设置权重损失；\n",
    "    weight1=variable_with_weight_loss(shape=[5,5,3,64],stddev=5e-2,wl=0.0, name='weight1')\n",
    "    kernel1=tf.nn.conv2d(image_holder,weight1,[1,1,1,1],padding='SAME')\n",
    "    bias1=tf.Variable(tf.constant(0.0,shape=[64]), name='bias1')\n",
    "    conv1=tf.nn.relu(tf.nn.bias_add(kernel1,bias1))\n",
    "    pool1=tf.nn.max_pool(conv1,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME')\n",
    "    #对上面处理后的记过进行LRN处理。\n",
    "    norm1=tf.nn.lrn(pool1,4,bias=1.0,alpha=0.001/9.0,beta=0.75)\n",
    "    \n",
    "    \n",
    "    ###创建第二个卷积层###这一层有64个节点\n",
    "    weight2=variable_with_weight_loss(shape=[5,5,64,64],stddev=5e-2,wl=0.0, name='weight2')\n",
    "    kernel2=tf.nn.conv2d(norm1,weight2,[1,1,1,1],padding='SAME')\n",
    "    bias2=tf.Variable(tf.constant(0.1,shape=[64]), name='bias2')\n",
    "    conv2=tf.nn.relu(tf.nn.bias_add(kernel2,bias2))\n",
    "    ###先进行LRN处理再做最大值池化\n",
    "    norm2=tf.nn.lrn(conv2,4,bias=1.0,alpha=0.001/9.0,beta=0.75)\n",
    "    pool2=tf.nn.max_pool(norm2,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "    ###创建全连接层### 这一层有384个节点\n",
    "    #首先将图像展开，这里的维度第一个是多少张图，第二个是根据图像大小确定的；然后使用get_shape()获取第二个维度，也就是图像大小\n",
    "    reshape=tf.reshape(pool2,[batch_size,-1])\n",
    "    dim=reshape.get_shape()[1].value\n",
    "\n",
    "    ###第一个全连接层，有384个节点###\n",
    "    weight3=variable_with_weight_loss(shape=[dim,384],stddev=0.04,wl=0.004, name='weight3')\n",
    "    bias3=tf.Variable(tf.constant(0.1,shape=[384]), name='bias3')\n",
    "    local3=tf.nn.relu(tf.matmul(reshape,weight3)+bias3)\n",
    "     \n",
    "    ###第二个全连接层，有192个节点\n",
    "    weight4=variable_with_weight_loss(shape=[384,192],stddev=0.04,wl=0.004, name='weight4')\n",
    "    bias4=tf.Variable(tf.constant(0.1,shape=[192]), name='bias4')\n",
    "    local4=tf.nn.relu(tf.matmul(local3,weight4)+bias4)\n",
    "\n",
    "    ###第三个全连接层，有10个节点\n",
    "    weight5=variable_with_weight_loss(shape=[192,10],stddev=1/192.0,wl=0.0, name='weight5')\n",
    "    bias5=tf.Variable(tf.constant(0.0,shape=[10]), name='bias5')\n",
    "    logits=tf.add(tf.matmul(local4,weight5),bias5, name=\"logits\")\n",
    "\n",
    "    \n",
    "    # Loss\n",
    "    loss = loss(logits, label_holder)\n",
    "    \n",
    "    #使用Adam Optimizer优化模型，学习率为0.001；\n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "    ##tf.nn.in_top_k()该函数首先将logits的输出排序，然后根据第三个参数选取排序的前N个，这里是1，就是选取第一个与label_holder进行对比；\n",
    "    ##比如logits最大的是第五个，而label_holder也是5；那么输出为True；\n",
    "    top_k_op = tf.nn.in_top_k(logits, label_holder, 1, name=\"top_k_op\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss = 7.05(41.4 examples/sec; 3.093 sec/batch)\n",
      "precision @ 0 = 0.078\n",
      "step 10, loss = 5.61(47.3 examples/sec; 2.708 sec/batch)\n",
      "precision @ 10 = 0.078\n",
      "step 20, loss = 4.94(47.2 examples/sec; 2.712 sec/batch)\n",
      "precision @ 20 = 0.164\n",
      "step 30, loss = 4.42(45.3 examples/sec; 2.826 sec/batch)\n",
      "precision @ 30 = 0.125\n",
      "step 40, loss = 3.94(43.1 examples/sec; 2.967 sec/batch)\n",
      "precision @ 40 = 0.211\n",
      "step 50, loss = 3.63(46.1 examples/sec; 2.777 sec/batch)\n",
      "precision @ 50 = 0.281\n",
      "step 60, loss = 3.34(47.5 examples/sec; 2.696 sec/batch)\n",
      "precision @ 60 = 0.328\n",
      "step 70, loss = 3.28(47.2 examples/sec; 2.713 sec/batch)\n",
      "precision @ 70 = 0.297\n",
      "step 80, loss = 3.04(45.4 examples/sec; 2.817 sec/batch)\n",
      "precision @ 80 = 0.359\n",
      "step 90, loss = 2.72(46.5 examples/sec; 2.750 sec/batch)\n",
      "precision @ 90 = 0.383\n",
      "step 100, loss = 2.59(47.3 examples/sec; 2.709 sec/batch)\n",
      "precision @ 100 = 0.367\n",
      "step 110, loss = 2.64(45.8 examples/sec; 2.795 sec/batch)\n",
      "precision @ 110 = 0.414\n",
      "step 120, loss = 2.56(48.1 examples/sec; 2.659 sec/batch)\n",
      "precision @ 120 = 0.438\n",
      "step 130, loss = 2.51(48.0 examples/sec; 2.669 sec/batch)\n",
      "precision @ 130 = 0.461\n",
      "step 140, loss = 2.37(48.0 examples/sec; 2.664 sec/batch)\n",
      "precision @ 140 = 0.461\n",
      "step 150, loss = 2.24(48.0 examples/sec; 2.666 sec/batch)\n",
      "precision @ 150 = 0.430\n",
      "step 160, loss = 2.36(46.6 examples/sec; 2.749 sec/batch)\n",
      "precision @ 160 = 0.469\n",
      "step 170, loss = 2.30(46.1 examples/sec; 2.777 sec/batch)\n",
      "precision @ 170 = 0.445\n",
      "step 180, loss = 2.01(48.1 examples/sec; 2.661 sec/batch)\n",
      "precision @ 180 = 0.492\n",
      "step 190, loss = 1.97(48.1 examples/sec; 2.659 sec/batch)\n",
      "precision @ 190 = 0.492\n",
      "step 200, loss = 2.12(48.4 examples/sec; 2.644 sec/batch)\n",
      "precision @ 200 = 0.539\n",
      "step 210, loss = 2.12(47.8 examples/sec; 2.677 sec/batch)\n",
      "precision @ 210 = 0.414\n",
      "step 220, loss = 1.97(30.9 examples/sec; 4.144 sec/batch)\n",
      "precision @ 220 = 0.523\n",
      "step 230, loss = 2.00(48.1 examples/sec; 2.663 sec/batch)\n",
      "precision @ 230 = 0.539\n",
      "step 240, loss = 1.96(47.6 examples/sec; 2.688 sec/batch)\n",
      "precision @ 240 = 0.453\n",
      "step 250, loss = 1.95(46.8 examples/sec; 2.737 sec/batch)\n",
      "precision @ 250 = 0.516\n",
      "step 260, loss = 1.90(47.5 examples/sec; 2.696 sec/batch)\n",
      "precision @ 260 = 0.492\n",
      "step 270, loss = 1.83(47.7 examples/sec; 2.685 sec/batch)\n",
      "precision @ 270 = 0.562\n",
      "step 280, loss = 1.86(48.2 examples/sec; 2.657 sec/batch)\n",
      "precision @ 280 = 0.586\n",
      "step 290, loss = 1.89(48.2 examples/sec; 2.654 sec/batch)\n",
      "precision @ 290 = 0.539\n",
      "step 300, loss = 1.60(46.0 examples/sec; 2.781 sec/batch)\n",
      "precision @ 300 = 0.516\n",
      "step 310, loss = 1.85(46.7 examples/sec; 2.743 sec/batch)\n",
      "precision @ 310 = 0.555\n",
      "step 320, loss = 1.86(47.6 examples/sec; 2.691 sec/batch)\n",
      "precision @ 320 = 0.484\n",
      "step 330, loss = 1.62(47.8 examples/sec; 2.680 sec/batch)\n",
      "precision @ 330 = 0.586\n",
      "step 340, loss = 1.64(47.4 examples/sec; 2.698 sec/batch)\n",
      "precision @ 340 = 0.562\n",
      "step 350, loss = 1.80(45.4 examples/sec; 2.820 sec/batch)\n",
      "precision @ 350 = 0.617\n",
      "step 360, loss = 1.38(47.6 examples/sec; 2.687 sec/batch)\n",
      "precision @ 360 = 0.555\n",
      "step 370, loss = 1.57(47.9 examples/sec; 2.670 sec/batch)\n",
      "precision @ 370 = 0.539\n",
      "step 380, loss = 1.78(48.4 examples/sec; 2.645 sec/batch)\n",
      "precision @ 380 = 0.539\n",
      "step 390, loss = 1.53(47.9 examples/sec; 2.671 sec/batch)\n",
      "precision @ 390 = 0.531\n",
      "step 400, loss = 1.63(47.5 examples/sec; 2.696 sec/batch)\n",
      "precision @ 400 = 0.547\n",
      "step 410, loss = 1.29(47.2 examples/sec; 2.711 sec/batch)\n",
      "precision @ 410 = 0.602\n",
      "step 420, loss = 1.58(47.7 examples/sec; 2.684 sec/batch)\n",
      "precision @ 420 = 0.594\n",
      "step 430, loss = 1.63(45.7 examples/sec; 2.798 sec/batch)\n",
      "precision @ 430 = 0.500\n",
      "step 440, loss = 1.42(46.5 examples/sec; 2.755 sec/batch)\n",
      "precision @ 440 = 0.633\n",
      "step 450, loss = 1.46(47.4 examples/sec; 2.700 sec/batch)\n",
      "precision @ 450 = 0.602\n",
      "step 460, loss = 1.58(48.1 examples/sec; 2.663 sec/batch)\n",
      "precision @ 460 = 0.625\n",
      "step 470, loss = 1.39(47.2 examples/sec; 2.712 sec/batch)\n",
      "precision @ 470 = 0.617\n",
      "step 480, loss = 1.46(48.0 examples/sec; 2.669 sec/batch)\n",
      "precision @ 480 = 0.531\n",
      "step 490, loss = 1.34(47.1 examples/sec; 2.718 sec/batch)\n",
      "precision @ 490 = 0.586\n",
      "step 500, loss = 1.49(47.4 examples/sec; 2.699 sec/batch)\n",
      "precision @ 500 = 0.586\n",
      "step 510, loss = 1.43(47.5 examples/sec; 2.693 sec/batch)\n",
      "precision @ 510 = 0.586\n",
      "step 520, loss = 1.37(47.6 examples/sec; 2.687 sec/batch)\n",
      "precision @ 520 = 0.625\n",
      "step 530, loss = 1.44(47.3 examples/sec; 2.708 sec/batch)\n",
      "precision @ 530 = 0.633\n",
      "step 540, loss = 1.35(48.0 examples/sec; 2.666 sec/batch)\n",
      "precision @ 540 = 0.609\n",
      "step 550, loss = 1.49(47.9 examples/sec; 2.671 sec/batch)\n",
      "precision @ 550 = 0.656\n",
      "step 560, loss = 1.44(47.2 examples/sec; 2.710 sec/batch)\n",
      "precision @ 560 = 0.625\n",
      "step 570, loss = 1.14(47.8 examples/sec; 2.677 sec/batch)\n",
      "precision @ 570 = 0.617\n",
      "step 580, loss = 1.30(47.5 examples/sec; 2.693 sec/batch)\n",
      "precision @ 580 = 0.648\n",
      "step 590, loss = 1.42(46.7 examples/sec; 2.740 sec/batch)\n",
      "precision @ 590 = 0.609\n",
      "step 600, loss = 1.37(47.5 examples/sec; 2.694 sec/batch)\n",
      "precision @ 600 = 0.594\n",
      "step 610, loss = 1.20(48.1 examples/sec; 2.659 sec/batch)\n",
      "precision @ 610 = 0.648\n",
      "step 620, loss = 1.40(47.9 examples/sec; 2.671 sec/batch)\n",
      "precision @ 620 = 0.711\n",
      "step 630, loss = 1.43(47.5 examples/sec; 2.696 sec/batch)\n",
      "precision @ 630 = 0.617\n",
      "step 640, loss = 1.48(48.4 examples/sec; 2.644 sec/batch)\n",
      "precision @ 640 = 0.617\n",
      "step 650, loss = 1.35(48.1 examples/sec; 2.661 sec/batch)\n",
      "precision @ 650 = 0.625\n",
      "step 660, loss = 1.27(47.8 examples/sec; 2.677 sec/batch)\n",
      "precision @ 660 = 0.648\n",
      "step 670, loss = 1.47(48.1 examples/sec; 2.661 sec/batch)\n",
      "precision @ 670 = 0.625\n",
      "step 680, loss = 1.34(47.1 examples/sec; 2.720 sec/batch)\n",
      "precision @ 680 = 0.602\n",
      "step 690, loss = 1.14(44.8 examples/sec; 2.858 sec/batch)\n",
      "precision @ 690 = 0.656\n",
      "step 700, loss = 1.47(48.3 examples/sec; 2.648 sec/batch)\n",
      "precision @ 700 = 0.672\n",
      "step 710, loss = 1.31(46.9 examples/sec; 2.729 sec/batch)\n",
      "precision @ 710 = 0.641\n",
      "step 720, loss = 1.14(47.9 examples/sec; 2.675 sec/batch)\n",
      "precision @ 720 = 0.672\n",
      "step 730, loss = 1.28(47.5 examples/sec; 2.696 sec/batch)\n",
      "precision @ 730 = 0.617\n",
      "step 740, loss = 1.36(47.4 examples/sec; 2.699 sec/batch)\n",
      "precision @ 740 = 0.609\n",
      "step 750, loss = 1.11(45.8 examples/sec; 2.798 sec/batch)\n",
      "precision @ 750 = 0.680\n",
      "step 760, loss = 1.17(48.1 examples/sec; 2.662 sec/batch)\n",
      "precision @ 760 = 0.703\n",
      "step 770, loss = 1.16(47.5 examples/sec; 2.694 sec/batch)\n",
      "precision @ 770 = 0.625\n",
      "step 780, loss = 1.12(47.8 examples/sec; 2.679 sec/batch)\n",
      "precision @ 780 = 0.641\n",
      "step 790, loss = 1.27(44.7 examples/sec; 2.864 sec/batch)\n",
      "precision @ 790 = 0.648\n",
      "step 800, loss = 1.01(47.8 examples/sec; 2.679 sec/batch)\n",
      "precision @ 800 = 0.633\n",
      "step 810, loss = 1.21(47.5 examples/sec; 2.695 sec/batch)\n",
      "precision @ 810 = 0.609\n",
      "step 820, loss = 1.29(47.2 examples/sec; 2.710 sec/batch)\n",
      "precision @ 820 = 0.680\n",
      "step 830, loss = 1.11(47.3 examples/sec; 2.709 sec/batch)\n",
      "precision @ 830 = 0.695\n",
      "step 840, loss = 1.23(47.7 examples/sec; 2.686 sec/batch)\n",
      "precision @ 840 = 0.609\n",
      "step 850, loss = 1.29(47.5 examples/sec; 2.692 sec/batch)\n",
      "precision @ 850 = 0.703\n",
      "step 860, loss = 1.22(45.8 examples/sec; 2.795 sec/batch)\n",
      "precision @ 860 = 0.688\n",
      "step 870, loss = 1.20(46.6 examples/sec; 2.748 sec/batch)\n",
      "precision @ 870 = 0.602\n",
      "step 880, loss = 1.15(47.7 examples/sec; 2.685 sec/batch)\n",
      "precision @ 880 = 0.641\n",
      "step 890, loss = 1.21(47.9 examples/sec; 2.671 sec/batch)\n",
      "precision @ 890 = 0.688\n",
      "step 900, loss = 1.21(48.1 examples/sec; 2.662 sec/batch)\n",
      "precision @ 900 = 0.695\n",
      "step 910, loss = 1.14(46.8 examples/sec; 2.737 sec/batch)\n",
      "precision @ 910 = 0.672\n",
      "step 920, loss = 1.31(47.6 examples/sec; 2.687 sec/batch)\n",
      "precision @ 920 = 0.664\n",
      "step 930, loss = 1.17(46.2 examples/sec; 2.768 sec/batch)\n",
      "precision @ 930 = 0.609\n",
      "step 940, loss = 1.22(48.0 examples/sec; 2.669 sec/batch)\n",
      "precision @ 940 = 0.656\n",
      "step 950, loss = 1.15(45.4 examples/sec; 2.822 sec/batch)\n",
      "precision @ 950 = 0.625\n",
      "step 960, loss = 0.98(47.2 examples/sec; 2.712 sec/batch)\n",
      "precision @ 960 = 0.656\n",
      "step 970, loss = 1.13(47.5 examples/sec; 2.695 sec/batch)\n",
      "precision @ 970 = 0.695\n",
      "step 980, loss = 1.21(46.9 examples/sec; 2.727 sec/batch)\n",
      "precision @ 980 = 0.641\n",
      "step 990, loss = 1.16(44.5 examples/sec; 2.876 sec/batch)\n",
      "precision @ 990 = 0.711\n",
      "step 1000, loss = 1.10(46.7 examples/sec; 2.741 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision @ 1000 = 0.664\n",
      "step 1010, loss = 1.20(47.9 examples/sec; 2.671 sec/batch)\n",
      "precision @ 1010 = 0.711\n",
      "step 1020, loss = 1.25(48.3 examples/sec; 2.649 sec/batch)\n",
      "precision @ 1020 = 0.641\n",
      "step 1030, loss = 1.21(48.0 examples/sec; 2.667 sec/batch)\n",
      "precision @ 1030 = 0.719\n",
      "step 1040, loss = 1.07(47.4 examples/sec; 2.700 sec/batch)\n",
      "precision @ 1040 = 0.656\n",
      "step 1050, loss = 1.23(47.5 examples/sec; 2.694 sec/batch)\n",
      "precision @ 1050 = 0.695\n",
      "step 1060, loss = 1.34(47.8 examples/sec; 2.675 sec/batch)\n",
      "precision @ 1060 = 0.633\n",
      "step 1070, loss = 1.26(48.1 examples/sec; 2.660 sec/batch)\n",
      "precision @ 1070 = 0.680\n",
      "step 1080, loss = 1.06(47.4 examples/sec; 2.700 sec/batch)\n",
      "precision @ 1080 = 0.594\n",
      "step 1090, loss = 1.22(47.8 examples/sec; 2.676 sec/batch)\n",
      "precision @ 1090 = 0.672\n",
      "step 1100, loss = 1.24(47.9 examples/sec; 2.670 sec/batch)\n",
      "precision @ 1100 = 0.617\n",
      "step 1110, loss = 0.99(47.5 examples/sec; 2.695 sec/batch)\n",
      "precision @ 1110 = 0.656\n",
      "step 1120, loss = 1.10(46.8 examples/sec; 2.736 sec/batch)\n",
      "precision @ 1120 = 0.648\n",
      "step 1130, loss = 1.27(47.1 examples/sec; 2.718 sec/batch)\n",
      "precision @ 1130 = 0.672\n",
      "step 1140, loss = 0.94(47.1 examples/sec; 2.719 sec/batch)\n",
      "precision @ 1140 = 0.688\n",
      "step 1150, loss = 1.01(47.9 examples/sec; 2.672 sec/batch)\n",
      "precision @ 1150 = 0.672\n",
      "step 1160, loss = 1.01(48.1 examples/sec; 2.660 sec/batch)\n",
      "precision @ 1160 = 0.633\n",
      "step 1170, loss = 0.97(47.7 examples/sec; 2.682 sec/batch)\n",
      "precision @ 1170 = 0.672\n",
      "step 1180, loss = 1.07(48.0 examples/sec; 2.669 sec/batch)\n",
      "precision @ 1180 = 0.664\n",
      "step 1190, loss = 0.86(47.8 examples/sec; 2.676 sec/batch)\n",
      "precision @ 1190 = 0.711\n",
      "step 1200, loss = 1.14(47.0 examples/sec; 2.724 sec/batch)\n",
      "precision @ 1200 = 0.680\n",
      "step 1210, loss = 1.19(47.0 examples/sec; 2.724 sec/batch)\n",
      "precision @ 1210 = 0.680\n",
      "step 1220, loss = 1.16(48.3 examples/sec; 2.648 sec/batch)\n",
      "precision @ 1220 = 0.672\n",
      "step 1230, loss = 1.04(46.9 examples/sec; 2.730 sec/batch)\n",
      "precision @ 1230 = 0.672\n",
      "step 1240, loss = 1.26(47.9 examples/sec; 2.673 sec/batch)\n",
      "precision @ 1240 = 0.695\n",
      "step 1250, loss = 1.13(47.6 examples/sec; 2.686 sec/batch)\n",
      "precision @ 1250 = 0.695\n",
      "step 1260, loss = 1.11(46.7 examples/sec; 2.740 sec/batch)\n",
      "precision @ 1260 = 0.656\n",
      "step 1270, loss = 1.09(48.1 examples/sec; 2.661 sec/batch)\n",
      "precision @ 1270 = 0.688\n",
      "step 1280, loss = 1.11(47.7 examples/sec; 2.684 sec/batch)\n",
      "precision @ 1280 = 0.688\n",
      "step 1290, loss = 1.09(46.2 examples/sec; 2.773 sec/batch)\n",
      "precision @ 1290 = 0.688\n",
      "step 1300, loss = 1.05(46.8 examples/sec; 2.737 sec/batch)\n",
      "precision @ 1300 = 0.680\n",
      "step 1310, loss = 1.21(47.4 examples/sec; 2.701 sec/batch)\n",
      "precision @ 1310 = 0.688\n",
      "step 1320, loss = 1.06(47.1 examples/sec; 2.717 sec/batch)\n",
      "precision @ 1320 = 0.664\n",
      "step 1330, loss = 1.13(47.2 examples/sec; 2.711 sec/batch)\n",
      "precision @ 1330 = 0.711\n",
      "step 1340, loss = 1.16(47.1 examples/sec; 2.719 sec/batch)\n",
      "precision @ 1340 = 0.648\n",
      "step 1350, loss = 0.94(48.2 examples/sec; 2.658 sec/batch)\n",
      "precision @ 1350 = 0.711\n",
      "step 1360, loss = 1.10(46.8 examples/sec; 2.735 sec/batch)\n",
      "precision @ 1360 = 0.727\n",
      "step 1370, loss = 1.30(48.0 examples/sec; 2.666 sec/batch)\n",
      "precision @ 1370 = 0.703\n",
      "step 1380, loss = 1.15(47.1 examples/sec; 2.718 sec/batch)\n",
      "precision @ 1380 = 0.633\n",
      "step 1390, loss = 0.99(46.5 examples/sec; 2.756 sec/batch)\n",
      "precision @ 1390 = 0.695\n",
      "step 1400, loss = 0.96(48.4 examples/sec; 2.642 sec/batch)\n",
      "precision @ 1400 = 0.727\n",
      "step 1410, loss = 1.21(48.3 examples/sec; 2.652 sec/batch)\n",
      "precision @ 1410 = 0.695\n",
      "step 1420, loss = 1.13(47.4 examples/sec; 2.700 sec/batch)\n",
      "precision @ 1420 = 0.664\n",
      "step 1430, loss = 0.98(47.0 examples/sec; 2.723 sec/batch)\n",
      "precision @ 1430 = 0.734\n",
      "step 1440, loss = 1.05(47.8 examples/sec; 2.680 sec/batch)\n",
      "precision @ 1440 = 0.688\n",
      "step 1450, loss = 1.21(45.4 examples/sec; 2.820 sec/batch)\n",
      "precision @ 1450 = 0.664\n",
      "step 1460, loss = 1.26(47.8 examples/sec; 2.676 sec/batch)\n",
      "precision @ 1460 = 0.680\n",
      "step 1470, loss = 1.01(47.1 examples/sec; 2.720 sec/batch)\n",
      "precision @ 1470 = 0.625\n",
      "step 1480, loss = 1.17(47.5 examples/sec; 2.692 sec/batch)\n",
      "precision @ 1480 = 0.719\n",
      "step 1490, loss = 1.07(47.6 examples/sec; 2.691 sec/batch)\n",
      "precision @ 1490 = 0.711\n",
      "step 1500, loss = 0.87(47.9 examples/sec; 2.672 sec/batch)\n",
      "precision @ 1500 = 0.672\n",
      "step 1510, loss = 0.95(44.6 examples/sec; 2.872 sec/batch)\n",
      "precision @ 1510 = 0.680\n",
      "step 1520, loss = 1.18(46.5 examples/sec; 2.754 sec/batch)\n",
      "precision @ 1520 = 0.680\n",
      "step 1530, loss = 0.93(48.2 examples/sec; 2.658 sec/batch)\n",
      "precision @ 1530 = 0.680\n",
      "step 1540, loss = 1.04(46.9 examples/sec; 2.728 sec/batch)\n",
      "precision @ 1540 = 0.688\n",
      "step 1550, loss = 1.00(47.9 examples/sec; 2.673 sec/batch)\n",
      "precision @ 1550 = 0.742\n",
      "step 1560, loss = 0.97(47.5 examples/sec; 2.694 sec/batch)\n",
      "precision @ 1560 = 0.672\n",
      "step 1570, loss = 1.09(47.0 examples/sec; 2.721 sec/batch)\n",
      "precision @ 1570 = 0.648\n",
      "step 1580, loss = 0.84(47.9 examples/sec; 2.671 sec/batch)\n",
      "precision @ 1580 = 0.688\n",
      "step 1590, loss = 1.10(47.5 examples/sec; 2.692 sec/batch)\n",
      "precision @ 1590 = 0.703\n",
      "step 1600, loss = 1.12(46.1 examples/sec; 2.774 sec/batch)\n",
      "precision @ 1600 = 0.727\n",
      "step 1610, loss = 1.03(47.8 examples/sec; 2.677 sec/batch)\n",
      "precision @ 1610 = 0.734\n",
      "step 1620, loss = 0.98(48.1 examples/sec; 2.663 sec/batch)\n",
      "precision @ 1620 = 0.766\n",
      "step 1630, loss = 1.19(47.7 examples/sec; 2.686 sec/batch)\n",
      "precision @ 1630 = 0.703\n",
      "step 1640, loss = 1.08(45.3 examples/sec; 2.827 sec/batch)\n",
      "precision @ 1640 = 0.727\n",
      "step 1650, loss = 1.08(48.0 examples/sec; 2.669 sec/batch)\n",
      "precision @ 1650 = 0.719\n",
      "step 1660, loss = 1.03(47.9 examples/sec; 2.671 sec/batch)\n",
      "precision @ 1660 = 0.727\n",
      "step 1670, loss = 1.13(47.0 examples/sec; 2.724 sec/batch)\n",
      "precision @ 1670 = 0.727\n",
      "step 1680, loss = 0.94(48.5 examples/sec; 2.640 sec/batch)\n",
      "precision @ 1680 = 0.680\n",
      "step 1690, loss = 1.04(47.8 examples/sec; 2.677 sec/batch)\n",
      "precision @ 1690 = 0.656\n",
      "step 1700, loss = 1.08(47.2 examples/sec; 2.713 sec/batch)\n",
      "precision @ 1700 = 0.648\n",
      "step 1710, loss = 1.00(47.3 examples/sec; 2.703 sec/batch)\n",
      "precision @ 1710 = 0.672\n",
      "step 1720, loss = 1.05(48.0 examples/sec; 2.668 sec/batch)\n",
      "precision @ 1720 = 0.742\n",
      "step 1730, loss = 1.21(46.8 examples/sec; 2.737 sec/batch)\n",
      "precision @ 1730 = 0.664\n",
      "step 1740, loss = 0.81(46.8 examples/sec; 2.733 sec/batch)\n",
      "precision @ 1740 = 0.727\n",
      "step 1750, loss = 1.06(47.7 examples/sec; 2.681 sec/batch)\n",
      "precision @ 1750 = 0.758\n",
      "step 1760, loss = 1.12(47.4 examples/sec; 2.701 sec/batch)\n",
      "precision @ 1760 = 0.766\n",
      "step 1770, loss = 1.17(43.5 examples/sec; 2.941 sec/batch)\n",
      "precision @ 1770 = 0.633\n",
      "step 1780, loss = 0.86(46.3 examples/sec; 2.765 sec/batch)\n",
      "precision @ 1780 = 0.664\n",
      "step 1790, loss = 0.91(48.3 examples/sec; 2.649 sec/batch)\n",
      "precision @ 1790 = 0.727\n",
      "step 1800, loss = 1.15(46.3 examples/sec; 2.762 sec/batch)\n",
      "precision @ 1800 = 0.680\n",
      "step 1810, loss = 1.05(47.6 examples/sec; 2.691 sec/batch)\n",
      "precision @ 1810 = 0.695\n",
      "step 1820, loss = 0.95(47.7 examples/sec; 2.681 sec/batch)\n",
      "precision @ 1820 = 0.711\n",
      "step 1830, loss = 0.97(46.5 examples/sec; 2.754 sec/batch)\n",
      "precision @ 1830 = 0.688\n",
      "step 1840, loss = 1.11(48.4 examples/sec; 2.646 sec/batch)\n",
      "precision @ 1840 = 0.664\n",
      "step 1850, loss = 1.13(47.7 examples/sec; 2.682 sec/batch)\n",
      "precision @ 1850 = 0.656\n",
      "step 1860, loss = 0.95(46.9 examples/sec; 2.728 sec/batch)\n",
      "precision @ 1860 = 0.680\n",
      "step 1870, loss = 1.08(47.0 examples/sec; 2.721 sec/batch)\n",
      "precision @ 1870 = 0.695\n",
      "step 1880, loss = 1.03(47.4 examples/sec; 2.702 sec/batch)\n",
      "precision @ 1880 = 0.734\n",
      "step 1890, loss = 0.87(46.6 examples/sec; 2.747 sec/batch)\n",
      "precision @ 1890 = 0.711\n",
      "step 1900, loss = 0.91(45.0 examples/sec; 2.842 sec/batch)\n",
      "precision @ 1900 = 0.633\n",
      "step 1910, loss = 1.19(47.7 examples/sec; 2.686 sec/batch)\n",
      "precision @ 1910 = 0.742\n",
      "step 1920, loss = 0.82(48.1 examples/sec; 2.661 sec/batch)\n",
      "precision @ 1920 = 0.703\n",
      "step 1930, loss = 0.96(47.8 examples/sec; 2.677 sec/batch)\n",
      "precision @ 1930 = 0.711\n",
      "step 1940, loss = 0.93(48.1 examples/sec; 2.660 sec/batch)\n",
      "precision @ 1940 = 0.719\n",
      "step 1950, loss = 0.92(47.5 examples/sec; 2.695 sec/batch)\n",
      "precision @ 1950 = 0.664\n",
      "step 1960, loss = 1.05(46.4 examples/sec; 2.761 sec/batch)\n",
      "precision @ 1960 = 0.656\n",
      "step 1970, loss = 0.73(48.7 examples/sec; 2.628 sec/batch)\n",
      "precision @ 1970 = 0.648\n",
      "step 1980, loss = 1.04(47.8 examples/sec; 2.678 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision @ 1980 = 0.680\n",
      "step 1990, loss = 1.08(47.8 examples/sec; 2.677 sec/batch)\n",
      "precision @ 1990 = 0.719\n",
      "step 2000, loss = 1.02(47.5 examples/sec; 2.694 sec/batch)\n",
      "precision @ 2000 = 0.703\n",
      "step 2010, loss = 0.99(47.4 examples/sec; 2.699 sec/batch)\n",
      "precision @ 2010 = 0.727\n",
      "step 2020, loss = 1.17(47.5 examples/sec; 2.694 sec/batch)\n",
      "precision @ 2020 = 0.711\n",
      "step 2030, loss = 1.02(46.2 examples/sec; 2.771 sec/batch)\n",
      "precision @ 2030 = 0.703\n",
      "step 2040, loss = 0.94(46.0 examples/sec; 2.784 sec/batch)\n",
      "precision @ 2040 = 0.727\n",
      "step 2050, loss = 0.86(47.9 examples/sec; 2.674 sec/batch)\n",
      "precision @ 2050 = 0.711\n",
      "step 2060, loss = 0.99(47.0 examples/sec; 2.726 sec/batch)\n",
      "precision @ 2060 = 0.703\n",
      "step 2070, loss = 0.87(47.6 examples/sec; 2.686 sec/batch)\n",
      "precision @ 2070 = 0.734\n",
      "step 2080, loss = 1.08(47.3 examples/sec; 2.706 sec/batch)\n",
      "precision @ 2080 = 0.672\n",
      "step 2090, loss = 1.00(46.5 examples/sec; 2.753 sec/batch)\n",
      "precision @ 2090 = 0.734\n",
      "step 2100, loss = 1.00(47.8 examples/sec; 2.676 sec/batch)\n",
      "precision @ 2100 = 0.633\n",
      "step 2110, loss = 0.93(48.2 examples/sec; 2.655 sec/batch)\n",
      "precision @ 2110 = 0.734\n",
      "step 2120, loss = 1.00(46.6 examples/sec; 2.746 sec/batch)\n",
      "precision @ 2120 = 0.641\n",
      "step 2130, loss = 0.79(46.8 examples/sec; 2.736 sec/batch)\n",
      "precision @ 2130 = 0.734\n",
      "step 2140, loss = 0.95(48.1 examples/sec; 2.659 sec/batch)\n",
      "precision @ 2140 = 0.719\n",
      "step 2150, loss = 1.13(47.4 examples/sec; 2.700 sec/batch)\n",
      "precision @ 2150 = 0.742\n",
      "step 2160, loss = 1.02(46.4 examples/sec; 2.757 sec/batch)\n",
      "precision @ 2160 = 0.656\n",
      "step 2170, loss = 0.82(47.3 examples/sec; 2.704 sec/batch)\n",
      "precision @ 2170 = 0.766\n",
      "step 2180, loss = 0.86(47.8 examples/sec; 2.676 sec/batch)\n",
      "precision @ 2180 = 0.727\n",
      "step 2190, loss = 1.15(47.1 examples/sec; 2.717 sec/batch)\n",
      "precision @ 2190 = 0.727\n",
      "step 2200, loss = 0.96(47.9 examples/sec; 2.671 sec/batch)\n",
      "precision @ 2200 = 0.758\n",
      "step 2210, loss = 0.90(47.8 examples/sec; 2.676 sec/batch)\n",
      "precision @ 2210 = 0.734\n",
      "step 2220, loss = 0.90(47.0 examples/sec; 2.725 sec/batch)\n",
      "precision @ 2220 = 0.727\n",
      "step 2230, loss = 1.07(48.5 examples/sec; 2.638 sec/batch)\n",
      "precision @ 2230 = 0.703\n",
      "step 2240, loss = 1.03(46.2 examples/sec; 2.771 sec/batch)\n",
      "precision @ 2240 = 0.695\n",
      "step 2250, loss = 0.96(45.2 examples/sec; 2.829 sec/batch)\n",
      "precision @ 2250 = 0.711\n",
      "step 2260, loss = 1.04(47.5 examples/sec; 2.692 sec/batch)\n",
      "precision @ 2260 = 0.688\n",
      "step 2270, loss = 1.04(46.9 examples/sec; 2.727 sec/batch)\n",
      "precision @ 2270 = 0.719\n",
      "step 2280, loss = 0.82(47.3 examples/sec; 2.708 sec/batch)\n",
      "precision @ 2280 = 0.672\n",
      "step 2290, loss = 1.02(46.7 examples/sec; 2.738 sec/batch)\n",
      "precision @ 2290 = 0.656\n",
      "step 2300, loss = 1.07(46.3 examples/sec; 2.764 sec/batch)\n",
      "precision @ 2300 = 0.680\n",
      "step 2310, loss = 0.71(48.2 examples/sec; 2.657 sec/batch)\n",
      "precision @ 2310 = 0.789\n",
      "step 2320, loss = 0.93(48.1 examples/sec; 2.659 sec/batch)\n",
      "precision @ 2320 = 0.711\n",
      "step 2330, loss = 0.91(48.1 examples/sec; 2.659 sec/batch)\n",
      "precision @ 2330 = 0.711\n",
      "step 2340, loss = 0.93(47.8 examples/sec; 2.680 sec/batch)\n",
      "precision @ 2340 = 0.703\n",
      "step 2350, loss = 0.97(47.0 examples/sec; 2.722 sec/batch)\n",
      "precision @ 2350 = 0.695\n",
      "step 2360, loss = 0.66(47.7 examples/sec; 2.685 sec/batch)\n",
      "precision @ 2360 = 0.711\n",
      "step 2370, loss = 1.05(48.4 examples/sec; 2.642 sec/batch)\n",
      "precision @ 2370 = 0.656\n",
      "step 2380, loss = 1.09(47.3 examples/sec; 2.706 sec/batch)\n",
      "precision @ 2380 = 0.680\n",
      "step 2390, loss = 0.93(48.0 examples/sec; 2.667 sec/batch)\n",
      "precision @ 2390 = 0.688\n",
      "step 2400, loss = 0.96(45.9 examples/sec; 2.789 sec/batch)\n",
      "precision @ 2400 = 0.742\n",
      "step 2410, loss = 1.15(47.0 examples/sec; 2.723 sec/batch)\n",
      "precision @ 2410 = 0.711\n",
      "step 2420, loss = 0.99(45.8 examples/sec; 2.798 sec/batch)\n",
      "precision @ 2420 = 0.695\n",
      "step 2430, loss = 0.96(47.4 examples/sec; 2.700 sec/batch)\n",
      "precision @ 2430 = 0.727\n",
      "step 2440, loss = 0.87(45.9 examples/sec; 2.788 sec/batch)\n",
      "precision @ 2440 = 0.695\n",
      "step 2450, loss = 0.97(47.6 examples/sec; 2.691 sec/batch)\n",
      "precision @ 2450 = 0.719\n",
      "step 2460, loss = 0.80(47.4 examples/sec; 2.702 sec/batch)\n",
      "precision @ 2460 = 0.719\n",
      "step 2470, loss = 0.93(47.9 examples/sec; 2.673 sec/batch)\n",
      "precision @ 2470 = 0.680\n",
      "step 2480, loss = 0.96(47.9 examples/sec; 2.675 sec/batch)\n",
      "precision @ 2480 = 0.688\n",
      "step 2490, loss = 0.95(48.2 examples/sec; 2.656 sec/batch)\n",
      "precision @ 2490 = 0.734\n",
      "step 2500, loss = 0.85(46.1 examples/sec; 2.777 sec/batch)\n",
      "precision @ 2500 = 0.742\n",
      "step 2510, loss = 0.91(47.5 examples/sec; 2.692 sec/batch)\n",
      "precision @ 2510 = 0.672\n",
      "step 2520, loss = 0.82(47.2 examples/sec; 2.711 sec/batch)\n",
      "precision @ 2520 = 0.719\n",
      "step 2530, loss = 0.89(48.3 examples/sec; 2.652 sec/batch)\n",
      "precision @ 2530 = 0.711\n",
      "step 2540, loss = 1.10(47.9 examples/sec; 2.672 sec/batch)\n",
      "precision @ 2540 = 0.766\n",
      "step 2550, loss = 1.04(46.1 examples/sec; 2.774 sec/batch)\n",
      "precision @ 2550 = 0.727\n",
      "step 2560, loss = 0.78(47.0 examples/sec; 2.724 sec/batch)\n",
      "precision @ 2560 = 0.719\n",
      "step 2570, loss = 0.81(46.8 examples/sec; 2.734 sec/batch)\n",
      "precision @ 2570 = 0.734\n",
      "step 2580, loss = 1.09(47.3 examples/sec; 2.704 sec/batch)\n",
      "precision @ 2580 = 0.711\n",
      "step 2590, loss = 1.01(47.7 examples/sec; 2.682 sec/batch)\n",
      "precision @ 2590 = 0.688\n",
      "step 2600, loss = 0.94(48.0 examples/sec; 2.669 sec/batch)\n",
      "precision @ 2600 = 0.742\n",
      "step 2610, loss = 0.90(46.5 examples/sec; 2.750 sec/batch)\n",
      "precision @ 2610 = 0.680\n",
      "step 2620, loss = 1.01(48.0 examples/sec; 2.665 sec/batch)\n",
      "precision @ 2620 = 0.695\n",
      "step 2630, loss = 1.02(47.2 examples/sec; 2.711 sec/batch)\n",
      "precision @ 2630 = 0.688\n",
      "step 2640, loss = 0.92(47.1 examples/sec; 2.720 sec/batch)\n",
      "precision @ 2640 = 0.719\n",
      "step 2650, loss = 1.05(47.6 examples/sec; 2.690 sec/batch)\n",
      "precision @ 2650 = 0.727\n",
      "step 2660, loss = 1.03(48.4 examples/sec; 2.646 sec/batch)\n",
      "precision @ 2660 = 0.750\n",
      "step 2670, loss = 0.73(46.6 examples/sec; 2.749 sec/batch)\n",
      "precision @ 2670 = 0.711\n",
      "step 2680, loss = 0.93(44.5 examples/sec; 2.874 sec/batch)\n",
      "precision @ 2680 = 0.672\n",
      "step 2690, loss = 1.05(47.8 examples/sec; 2.680 sec/batch)\n",
      "precision @ 2690 = 0.633\n",
      "step 2700, loss = 0.74(48.4 examples/sec; 2.643 sec/batch)\n",
      "precision @ 2700 = 0.742\n",
      "step 2710, loss = 0.90(48.2 examples/sec; 2.658 sec/batch)\n",
      "precision @ 2710 = 0.711\n",
      "step 2720, loss = 0.83(47.6 examples/sec; 2.691 sec/batch)\n",
      "precision @ 2720 = 0.727\n",
      "step 2730, loss = 0.81(48.0 examples/sec; 2.668 sec/batch)\n",
      "precision @ 2730 = 0.727\n",
      "step 2740, loss = 0.98(47.2 examples/sec; 2.713 sec/batch)\n",
      "precision @ 2740 = 0.734\n",
      "step 2750, loss = 0.64(47.5 examples/sec; 2.695 sec/batch)\n",
      "precision @ 2750 = 0.680\n",
      "step 2760, loss = 0.92(47.4 examples/sec; 2.700 sec/batch)\n",
      "precision @ 2760 = 0.664\n",
      "step 2770, loss = 1.04(47.4 examples/sec; 2.703 sec/batch)\n",
      "precision @ 2770 = 0.695\n",
      "step 2780, loss = 0.81(47.2 examples/sec; 2.713 sec/batch)\n",
      "precision @ 2780 = 0.672\n",
      "step 2790, loss = 0.86(48.3 examples/sec; 2.647 sec/batch)\n",
      "precision @ 2790 = 0.711\n",
      "step 2800, loss = 1.04(46.2 examples/sec; 2.770 sec/batch)\n",
      "precision @ 2800 = 0.719\n",
      "step 2810, loss = 0.88(44.6 examples/sec; 2.872 sec/batch)\n",
      "precision @ 2810 = 0.719\n",
      "step 2820, loss = 0.98(45.0 examples/sec; 2.843 sec/batch)\n",
      "precision @ 2820 = 0.711\n",
      "step 2830, loss = 0.83(47.9 examples/sec; 2.672 sec/batch)\n",
      "precision @ 2830 = 0.742\n",
      "step 2840, loss = 0.86(46.4 examples/sec; 2.761 sec/batch)\n",
      "precision @ 2840 = 0.664\n",
      "step 2850, loss = 0.77(47.8 examples/sec; 2.680 sec/batch)\n",
      "precision @ 2850 = 0.727\n",
      "step 2860, loss = 0.98(47.5 examples/sec; 2.697 sec/batch)\n",
      "precision @ 2860 = 0.703\n",
      "step 2870, loss = 0.92(47.0 examples/sec; 2.721 sec/batch)\n",
      "precision @ 2870 = 0.703\n",
      "step 2880, loss = 0.95(47.8 examples/sec; 2.677 sec/batch)\n",
      "precision @ 2880 = 0.727\n",
      "step 2890, loss = 0.82(46.9 examples/sec; 2.732 sec/batch)\n",
      "precision @ 2890 = 0.719\n",
      "step 2900, loss = 0.93(47.6 examples/sec; 2.689 sec/batch)\n",
      "precision @ 2900 = 0.656\n",
      "step 2910, loss = 0.77(46.6 examples/sec; 2.744 sec/batch)\n",
      "precision @ 2910 = 0.734\n",
      "step 2920, loss = 0.89(47.5 examples/sec; 2.694 sec/batch)\n",
      "precision @ 2920 = 0.758\n",
      "step 2930, loss = 1.01(47.3 examples/sec; 2.705 sec/batch)\n",
      "precision @ 2930 = 0.750\n",
      "step 2940, loss = 1.04(45.1 examples/sec; 2.835 sec/batch)\n",
      "precision @ 2940 = 0.719\n",
      "step 2950, loss = 0.75(47.9 examples/sec; 2.671 sec/batch)\n",
      "precision @ 2950 = 0.734\n",
      "step 2960, loss = 0.78(48.4 examples/sec; 2.642 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision @ 2960 = 0.719\n",
      "step 2970, loss = 1.02(46.7 examples/sec; 2.743 sec/batch)\n",
      "precision @ 2970 = 0.711\n",
      "step 2980, loss = 0.95(47.3 examples/sec; 2.706 sec/batch)\n",
      "precision @ 2980 = 0.695\n",
      "step 2990, loss = 0.95(47.7 examples/sec; 2.683 sec/batch)\n",
      "precision @ 2990 = 0.703\n",
      "INFO:tensorflow:Froze 10 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0902 21:59:20.671936 4483290560 graph_util_impl.py:268] Froze 10 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted 10 variables to const ops.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0902 21:59:20.692440 4483290560 graph_util_impl.py:301] Converted 10 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        start_time = time.time()\n",
    "        image_batch, label_batch = nextBatch()\n",
    "        _, loss_value = sess.run([train_op, loss], feed_dict = {image_holder: image_batch, label_holder: label_batch})\n",
    "        duration = time.time() - start_time\n",
    "        if step % 10 == 0:\n",
    "            examples_per_sec = batch_size / duration\n",
    "            sec_per_batch = float(duration)\n",
    "\n",
    "            format_str = ('step %d, loss = %.2f(%.1f examples/sec; %.3f sec/batch)')\n",
    "            print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))\n",
    "            \n",
    "            predictions = sess.run([top_k_op], feed_dict = {image_holder: testX[0:128], label_holder: np.reshape(testY[0:128], (128,))})\n",
    "            precision = np.sum(predictions) / float(predictions[0].shape[0])\n",
    "            print('precision @ %d = %.3f' % (step, precision))\n",
    "            \n",
    "    constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['logits', 'total_loss', 'top_k_op/top_k_op'])\n",
    "    \n",
    "    with tf.gfile.FastGFile(pb_file_path+'complex_cnn_cifar10_model.pb', mode='wb') as f:\n",
    "        f.write(constant_graph.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "##用含有10000张图片的测试集测试准确率，并且打印出来。    \n",
    "# num_examples = 10000\n",
    "# import math\n",
    " \n",
    "# num_iter = int(math.ceil(num_examples / batch_size))\n",
    "# true_count = 0\n",
    "# total_sample_count = num_iter * batch_size\n",
    "# step = 0\n",
    " \n",
    "# while step < num_iter:\n",
    "#     image_batch, label_batch = nextBatch()\n",
    "#     predictions = sess.run([top_k_op], feed_dict = {image_holder: image_batch, label_holder: label_batch})\n",
    "#     true_count += np.sum(predictions)\n",
    "#     step += 1\n",
    " \n",
    "# precision = true_count / total_sample_count\n",
    "# print('precision @ 1 = %.3f' % precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'weight1:0' shape=(5, 5, 3, 64) dtype=float32_ref>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
